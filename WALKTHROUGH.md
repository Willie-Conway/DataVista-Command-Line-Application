# ðŸš€ DataVista - Complete Code Walkthrough & User Guide

![DataVista](https://github.com/Willie-Conway/DataVista-App/blob/62b22806b37009186f100531f50769ed98517397/assets/DataVista.png)

![DataVista Walkthrough](https://img.shields.io/badge/ðŸ“š_Complete_Walkthrough-8B5CF6?style=for-the-badge&logo=bookstack&logoColor=white)
![Interactive Guide](https://img.shields.io/badge/ðŸŽ®_Interactive_Guide-00B894?style=for-the-badge&logo=gamecontroller&logoColor=white)
![Step-by-Step](https://img.shields.io/badge/ðŸ‘£_Step_by_Step-FF9F43?style=for-the-badge&logo=footsteps&logoColor=white)

## ðŸ“– Introduction

**DataVista** is a comprehensive Python-based data science platform that provides end-to-end analytics capabilities through an intuitive command-line interface. This walkthrough guide demonstrates every feature, module, and workflow available in DataVista, complete with code examples, outputs, and best practices.

## ðŸŽ¯ Quick Navigation


ðŸ” **Quick Links:**
- [Task 1: Statistical Analysis](#-task-1-statistical-analysis)
- [Task 2: Machine Learning](#-task-2-machine-learning)
- [Task 3: Data Visualization](#-task-3-data-visualization)
- [Task 4: Model Management](#-task-4-model-management)
- [Task 5: Clustering Analysis](#-task-5-clustering-analysis)
- [Task 6: Time Series Forecasting](#-task-6-time-series-forecasting)
- [Task 7: Hypothesis Testing](#-task-7-hypothesis-testing)
- [Task 8: Advanced Features](#-task-8-advanced-features)
- [Task 9: Integration & Automation](#-task-9-integration--automation)


## ðŸ—ï¸ Architecture Overview

```python
# Core Architecture Components
class DataVistaArchitecture:
    """
    DataVista follows a modular, extensible architecture:
    """
    modules = {
        'data_loader': 'Multi-format data ingestion',
        'data_cleaner': 'Data quality & preprocessing',
        'data_preprocessor': 'Feature engineering',
        'statistical_analysis': 'Descriptive & inferential stats',
        'hypothesis_testing': 'A/B testing & validation',
        'machine_learning': 'Supervised/unsupervised learning',
        'visualization': 'Interactive plotting engine',
        'model_generater': 'AutoML & model generation',
        'pipeline_manager': 'End-to-end workflow orchestration'
    }
    
    def get_workflow(self):
        return {
            'phase1': 'Data Ingestion & Validation',
            'phase2': 'Exploratory Data Analysis',
            'phase3': 'Feature Engineering',
            'phase4': 'Model Training & Evaluation',
            'phase5': 'Visualization & Reporting',
            'phase6': 'Deployment & Monitoring'
        }
```

## ðŸ“Š Task 1: Statistical Analysis

### **Comprehensive Statistical Workflow**

```python
# data_vista.py - Statistical Analysis Implementation
def perform_statistical_analysis(self, data: pd.DataFrame) -> dict:
    """
    Complete statistical analysis pipeline
    """
    import numpy as np
    from scipy import stats
    import warnings
    warnings.filterwarnings('ignore')
    
    analysis_results = {
        'descriptive_stats': self._calculate_descriptive_stats(data),
        'distribution_analysis': self._analyze_distributions(data),
        'correlation_analysis': self._calculate_correlations(data),
        'outlier_detection': self._detect_outliers(data),
        'normality_tests': self._perform_normality_tests(data),
        'advanced_metrics': self._calculate_advanced_metrics(data)
    }
    
    return analysis_results

def _calculate_descriptive_stats(self, data: pd.DataFrame) -> dict:
    """Calculate comprehensive descriptive statistics"""
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    
    stats = {}
    for col in numeric_cols:
        col_data = data[col].dropna()
        
        stats[col] = {
            'count': len(col_data),
            'mean': np.mean(col_data),
            'median': np.median(col_data),
            'mode': stats.mode(col_data, keepdims=True)[0][0],
            'std_dev': np.std(col_data),
            'variance': np.var(col_data),
            'min': np.min(col_data),
            'max': np.max(col_data),
            'range': np.ptp(col_data),
            'q1': np.percentile(col_data, 25),
            'q3': np.percentile(col_data, 75),
            'iqr': np.percentile(col_data, 75) - np.percentile(col_data, 25),
            'skewness': stats.skew(col_data),
            'kurtosis': stats.kurtosis(col_data),
            'coefficient_of_variation': (np.std(col_data) / np.mean(col_data)) * 100,
            'confidence_interval_95': self._calculate_confidence_interval(col_data, 0.95),
            'confidence_interval_99': self._calculate_confidence_interval(col_data, 0.99)
        }
    
    return stats

def _calculate_confidence_interval(self, data: pd.Series, 
                                  confidence: float = 0.95) -> tuple:
    """Calculate confidence interval using t-distribution"""
    n = len(data)
    mean = np.mean(data)
    std_err = stats.sem(data)
    
    # Calculate t-value for confidence level
    t_value = stats.t.ppf((1 + confidence) / 2, n - 1)
    
    margin_of_error = t_value * std_err
    ci_lower = mean - margin_of_error
    ci_upper = mean + margin_of_error
    
    return (ci_lower, ci_upper)
```

### **Command-Line Execution Example**

```bash
# Basic statistical analysis
python src/data_vista.py --data data/market_research.csv --analysis stats

# With detailed output
python src/data_vista.py \
  --data data/customer_churn.csv \
  --analysis detailed \
  --output stats_report.json \
  --format json

# Interactive mode
python src/data_vista.py --interactive
```

### **Sample Output**

```markdown
ðŸ“Š COMPREHENSIVE STATISTICAL ANALYSIS REPORT
============================================
Dataset: market_research.csv
Records: 1,000 | Features: 8
Analysis Timestamp: 2024-10-22 18:59:44

ðŸ“ˆ DESCRIPTIVE STATISTICS
-------------------------
Feature: price
â€¢ Count: 1,000
â€¢ Mean: $254.19 Â± $12.45 (95% CI)
â€¢ Median: $149.99
â€¢ Std Dev: $330.55
â€¢ Range: $974.00 ($25.99 - $999.99)
â€¢ IQR: $195.50 ($41.99 - $237.49)
â€¢ Skewness: 1.84 (Right-skewed)
â€¢ Kurtosis: 4.21 (Leptokurtic)

ðŸ“Š DISTRIBUTION ANALYSIS
------------------------
â€¢ Shapiro-Wilk Test: W=0.923, p=0.001 (Not normal)
â€¢ Kolmogorov-Smirnov: D=0.184, p<0.001 (Not normal)
â€¢ Anderson-Darling: AÂ²=8.42, Critical=0.787 (Not normal)

ðŸ”— CORRELATION MATRIX
---------------------
               price  market_share  customer_rating
price          1.000         0.741            0.312
market_share   0.741         1.000            0.456
customer_rating 0.312        0.456            1.000

ðŸŽ¯ OUTLIER DETECTION
-------------------
â€¢ IQR Method: 12 outliers detected (> $572.48)
â€¢ Z-score Method: 8 outliers detected (|z| > 3)
â€¢ Modified Z-score: 9 outliers detected

ðŸ“‹ RECOMMENDATIONS
-----------------
1. Consider log transformation for 'price' (high skewness)
2. Remove or winsorize outliers in 'market_share'
3. Strong correlation between price and market_share (r=0.74)
4. Consider dimensionality reduction for correlated features
```

## ðŸ¤– Task 2: Machine Learning

### **Advanced ML Pipeline Implementation**

```python
# machine_learning.py - Complete ML Pipeline
class AdvancedMLPipeline:
    """End-to-end machine learning with hyperparameter tuning"""
    
    def __init__(self):
        self.supported_algorithms = {
            'regression': ['linear', 'ridge', 'lasso', 'elasticnet',
                          'random_forest', 'gradient_boosting', 'xgboost',
                          'lightgbm', 'catboost'],
            'classification': ['logistic', 'random_forest', 'gradient_boosting',
                             'svm', 'knn', 'naive_bayes', 'xgboost',
                             'lightgbm', 'catboost'],
            'clustering': ['kmeans', 'dbscan', 'hierarchical', 'gaussian_mixture'],
            'dimensionality_reduction': ['pca', 'tsne', 'umap', 'lda']
        }
    
    def train_model(self, X: pd.DataFrame, y: pd.Series, 
                   task_type: str = 'auto') -> dict:
        """Automated model training with optimization"""
        
        # Auto-detect task type
        if task_type == 'auto':
            task_type = self._detect_task_type(y)
        
        # Feature engineering
        X_processed = self._feature_engineering(X, y, task_type)
        
        # Feature selection
        X_selected = self._feature_selection(X_processed, y, task_type)
        
        # Algorithm selection
        model = self._select_best_algorithm(X_selected, y, task_type)
        
        # Hyperparameter tuning
        best_params = self._hyperparameter_tuning(model, X_selected, y)
        
        # Train final model
        final_model = self._train_final_model(model.__class__, 
                                             best_params, 
                                             X_selected, y)
        
        # Model evaluation
        evaluation = self._evaluate_model(final_model, X_selected, y)
        
        # Feature importance
        importance = self._calculate_feature_importance(final_model, 
                                                       X_selected.columns)
        
        return {
            'model': final_model,
            'algorithm': model.__class__.__name__,
            'parameters': best_params,
            'evaluation': evaluation,
            'feature_importance': importance,
            'selected_features': list(X_selected.columns),
            'task_type': task_type
        }
    
    def _hyperparameter_tuning(self, model, X: pd.DataFrame, 
                              y: pd.Series) -> dict:
        """Advanced hyperparameter optimization"""
        from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
        import optuna
        
        # Define parameter grids based on algorithm
        param_grids = {
            'RandomForest': {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 20, 30, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'XGBoost': {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.3],
                'subsample': [0.8, 0.9, 1.0]
            }
        }
        
        # Use Optuna for advanced optimization
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)
            }
            
            model.set_params(**params)
            scores = cross_val_score(model, X, y, cv=5, 
                                    scoring='neg_mean_squared_error')
            return scores.mean()
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50)
        
        return study.best_params
```

### **Training Workflow Example**

```bash
# Train classification model
python src/data_vista.py \
  --data data/customer_churn.csv \
  --target churn \
  --algorithm random_forest \
  --tune hyperparameters \
  --cv 10 \
  --output model_results/

# Train with autoML
python src/data_vista.py \
  --data data/house_prices.csv \
  --target price \
  --automl \
  --time_limit 3600 \
  --ensemble

# Compare multiple algorithms
python src/data_vista.py \
  --data data/credit_risk.csv \
  --target risk \
  --compare algorithms \
  --algorithms random_forest xgboost lightgbm catboost \
  --metric roc_auc
```

### **ML Output Report**

```markdown
ðŸ¤– MACHINE LEARNING ANALYSIS REPORT
===================================
Dataset: customer_churn.csv
Target: churn (Binary Classification)
Records: 10,000 | Features: 20
Timestamp: 2024-10-22 20:01:41

ðŸŽ¯ ALGORITHM SELECTION
---------------------
Selected Algorithm: Random Forest Classifier
Reason: Best performance on validation set (AUC: 0.892)
Alternative: XGBoost (AUC: 0.889)

âš™ï¸ HYPERPARAMETER OPTIMIZATION
------------------------------
Optimal Parameters:
â€¢ n_estimators: 300
â€¢ max_depth: 15
â€¢ min_samples_split: 5
â€¢ min_samples_leaf: 2
â€¢ max_features: 'sqrt'
â€¢ bootstrap: True

ðŸ“Š MODEL PERFORMANCE
-------------------
Training Set:
â€¢ Accuracy: 0.912 Â± 0.008 (5-fold CV)
â€¢ Precision: 0.889
â€¢ Recall: 0.901
â€¢ F1-Score: 0.895
â€¢ AUC-ROC: 0.892

Test Set (Holdout):
â€¢ Accuracy: 0.901
â€¢ Precision: 0.876
â€¢ Recall: 0.892
â€¢ F1-Score: 0.884
â€¢ AUC-ROC: 0.887

ðŸ“ˆ LEARNING CURVES
------------------
â€¢ Training Score: Converged at 8,000 samples
â€¢ Validation Score: Stable after 7,000 samples
â€¢ Gap: 0.04 (Low overfitting)

ðŸ” FEATURE IMPORTANCE
--------------------
1. total_charges: 0.234 (Most important)
2. monthly_charges: 0.187
3. tenure: 0.156
4. contract_type: 0.098
5. online_security: 0.067
6. tech_support: 0.054
7. paperless_billing: 0.043
8. payment_method: 0.038
9. ... 12 more features

ðŸŽ¯ BUSINESS INSIGHTS
-------------------
1. Total charges is the strongest churn predictor
2. Tenure under 12 months has 3x higher churn risk
3. Month-to-month contracts have 40% higher churn
4. Customers without tech support churn 25% more

ðŸ’¾ MODEL ARTIFACTS
-----------------
Saved Files:
â€¢ model.joblib (Serialized model)
â€¢ feature_importance.png
â€¢ learning_curves.png
â€¢ confusion_matrix.png
â€¢ performance_report.json
```

## ðŸ“Š Task 3: Data Visualization

### **Advanced Visualization Engine**

```python
# visualization.py - Complete Visualization System
class AdvancedVisualization:
    """Professional-grade visualization system"""
    
    def __init__(self, style: str = 'seaborn', 
                 palette: str = 'viridis'):
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Set global styles
        plt.style.use(style)
        sns.set_palette(palette)
        plt.rcParams.update({
            'figure.figsize': (12, 8),
            'font.size': 11,
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'figure.dpi': 150,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight'
        })
        
        self.plt = plt
        self.sns = sns
    
    def create_comprehensive_dashboard(self, data: pd.DataFrame, 
                                      analysis_results: dict) -> None:
        """Create professional analytics dashboard"""
        
        fig = self.plt.figure(figsize=(20, 25))
        
        # 1. Data Overview
        ax1 = self.plt.subplot(5, 4, 1)
        self._plot_data_overview(data, ax1)
        
        # 2. Missing Values Heatmap
        ax2 = self.plt.subplot(5, 4, 2)
        self._plot_missing_values(data, ax2)
        
        # 3. Correlation Matrix
        ax3 = self.plt.subplot(5, 4, 3)
        self._plot_correlation_matrix(data, ax3)
        
        # 4. Distribution Grid
        ax4 = self.plt.subplot(5, 4, 4)
        self._plot_distribution_grid(data, ax4)
        
        # 5. Boxplot Matrix
        ax5 = self.plt.subplot(5, 4, (5, 6))
        self._plot_boxplot_matrix(data, ax5)
        
        # 6. Pairplot (Sample)
        ax6 = self.plt.subplot(5, 4, (7, 8))
        self._plot_pairplot_sample(data, ax6)
        
        # 7. Time Series (if applicable)
        if self._has_datetime(data):
            ax7 = self.plt.subplot(5, 4, (9, 10))
            self._plot_time_series(data, ax7)
        
        # 8. Categorical Analysis
        ax8 = self.plt.subplot(5, 4, (11, 12))
        self._plot_categorical_analysis(data, ax8)
        
        # 9. Outlier Detection
        ax9 = self.plt.subplot(5, 4, (13, 14))
        self._plot_outlier_detection(data, ax9)
        
        # 10. Feature Importance
        if 'feature_importance' in analysis_results:
            ax10 = self.plt.subplot(5, 4, (15, 16))
            self._plot_feature_importance(analysis_results['feature_importance'], ax10)
        
        # 11. Model Performance
        if 'model_performance' in analysis_results:
            ax11 = self.plt.subplot(5, 4, (17, 18))
            self._plot_model_performance(analysis_results['model_performance'], ax11)
        
        # 12. Residual Analysis
        if 'residuals' in analysis_results:
            ax12 = self.plt.subplot(5, 4, (19, 20))
            self._plot_residual_analysis(analysis_results['residuals'], ax12)
        
        self.plt.suptitle('DataVista - Comprehensive Analytics Dashboard', 
                         fontsize=16, fontweight='bold', y=1.02)
        self.plt.tight_layout()
        self.plt.savefig('datavista_dashboard.png', dpi=300, 
                        bbox_inches='tight', facecolor='white')
        self.plt.show()
    
    def _plot_interactive_chart(self, data: pd.DataFrame, 
                               chart_type: str, **kwargs):
        """Create interactive Plotly charts"""
        import plotly.express as px
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        chart_types = {
            'scatter': px.scatter,
            'line': px.line,
            'bar': px.bar,
            'histogram': px.histogram,
            'box': px.box,
            'violin': px.violin,
            'density_contour': px.density_contour,
            'density_heatmap': px.density_heatmap,
            'parallel_coordinates': px.parallel_coordinates,
            'parallel_categories': px.parallel_categories,
            'sunburst': px.sunburst,
            'treemap': px.treemap,
            'funnel': px.funnel,
            'funnel_area': px.funnel_area
        }
        
        if chart_type in chart_types:
            fig = chart_types[chart_type](data, **kwargs)
            fig.update_layout(
                template='plotly_white',
                font=dict(family="Arial", size=12),
                title_font=dict(size=16, family="Arial", color='darkblue'),
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                )
            )
            return fig
```

### **Visualization Examples**

```bash
# Create comprehensive dashboard
python src/data_vista.py \
  --data data/sales_data.csv \
  --visualize dashboard \
  --output dashboard.html \
  --interactive

# Create specific chart types
python src/data_vista.py \
  --data data/customer_data.csv \
  --visualize correlation \
  --method spearman \
  --annotate

# Create geographical plot
python src/data_vista.py \
  --data data/store_locations.csv \
  --visualize geospatial \
  --latitude lat_column \
  --longitude lon_column \
  --color sales

# Create time series decomposition
python src/data_vista.py \
  --data data/time_series.csv \
  --visualize decomposition \
  --period 12 \
  --model additive
```

### **Sample Visualization Output**

```markdown
ðŸ“Š DATA VISTA VISUALIZATION SUITE
=================================
Generated: 2024-10-22 19:06:33
Dataset: market_research.csv

ðŸŽ¨ CHART GALLERY
---------------
1. ðŸ“ˆ Distribution Plot (Category)
   â€¢ Type: Bar Chart
   â€¢ Colors: Set3 palette
   â€¢ Annotations: Value labels
   â€¢ File: bar_chart_category.png

2. ðŸ”¥ Correlation Heatmap
   â€¢ Method: Pearson correlation
   â€¢ Mask: Upper triangle
   â€¢ Annotations: 2 decimal places
   â€¢ File: correlation_heatmap.png

3. ðŸ“Š Feature Distribution Grid
   â€¢ Columns: price, market_share, rating
   â€¢ Charts: Histogram + KDE
   â€¢ Statistics: Mean, median lines
   â€¢ File: distribution_grid.png

4. ðŸŽ¯ Boxplot Matrix
   â€¢ Group by: category
   â€¢ Features: All numeric
   â€¢ Outliers: Highlighted
   â€¢ File: boxplot_matrix.png

5. ðŸ”— Pairplot Analysis
   â€¢ Features: Top 5 correlated
   â€¢ Diagonal: KDE plots
   â€¢ Hue: category
   â€¢ File: pairplot.png

6. ðŸ“… Time Series Analysis
   â€¢ Frequency: Monthly
   â€¢ Decomposition: Trend, Seasonality, Residual
   â€¢ Forecast: 12 periods
   â€¢ File: time_series.png

7. ðŸŽ¨ Interactive Dashboard
   â€¢ Format: HTML with Plotly
   â€¢ Features: Zoom, pan, hover
   â€¢ Export: PNG, PDF, SVG
   â€¢ File: interactive_dashboard.html

ðŸŽ¯ VISUALIZATION INSIGHTS
------------------------
1. Price distribution shows bimodal pattern
2. Strong positive correlation (0.74) between price and market share
3. Category "Electronics" has highest variance
4. Seasonal pattern detected with 6-month cycles
5. Outliers detected in premium product segment

ðŸ’¾ EXPORTED FILES
----------------
â€¢ PNG Images: 8 files (300 DPI)
â€¢ PDF Report: datavista_report.pdf
â€¢ HTML Dashboard: interactive_dashboard.html
â€¢ JSON Data: visualization_data.json
â€¢ Configuration: viz_config.yaml
```

## ðŸ’¾ Task 4: Model Management

### **Complete Model Management System**

```python
# model_generater.py - Model Serialization & Management
class ModelManager:
    """Professional model persistence and versioning"""
    
    def __init__(self, model_dir: str = 'models'):
        import joblib
        import pickle
        import json
        import yaml
        
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        self.serializers = {
            'joblib': joblib.dump,
            'pickle': pickle.dump,
            'json': self._save_json,
            'onnx': self._save_onnx,
            'pmml': self._save_pmml
        }
        
        self.deserializers = {
            'joblib': joblib.load,
            'pickle': pickle.load,
            'json': self._load_json,
            'onnx': self._load_onnx,
            'pmml': self._load_pmml
        }
    
    def save_model(self, model, model_name: str, 
                  metadata: dict = None, 
                  format: str = 'joblib') -> str:
        """Save model with comprehensive metadata"""
        
        # Create model directory
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = self.model_dir / f"{model_name}_{timestamp}"
        model_path.mkdir(exist_ok=True)
        
        # Save model
        model_file = model_path / f"model.{format}"
        self.serializers[format](model, model_file)
        
        # Save metadata
        metadata_file = model_path / "metadata.json"
        metadata = metadata or {}
        metadata.update({
            'model_name': model_name,
            'timestamp': timestamp,
            'format': format,
            'model_type': type(model).__name__,
            'model_params': model.get_params() if hasattr(model, 'get_params') else {},
            'python_version': sys.version,
            'library_versions': self._get_library_versions(),
            'performance_metrics': getattr(model, 'performance_metrics', {}),
            'feature_names': getattr(model, 'feature_names', [])
        })
        
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Save feature importance
        if hasattr(model, 'feature_importances_'):
            importance_file = model_path / "feature_importance.json"
            importance_data = {
                'feature_names': getattr(model, 'feature_names', []),
                'importances': model.feature_importances_.tolist()
            }
            with open(importance_file, 'w') as f:
                json.dump(importance_data, f, indent=2)
        
        # Save training data sample
        if hasattr(model, 'X_train_'):
            sample_file = model_path / "training_sample.csv"
            pd.DataFrame(model.X_train_).iloc[:100].to_csv(sample_file)
        
        # Create README
        readme_file = model_path / "README.md"
        self._create_model_readme(model, metadata, readme_file)
        
        return str(model_path)
    
    def load_model(self, model_path: str, 
                  format: str = 'auto') -> tuple:
        """Load model with validation"""
        
        model_path = Path(model_path)
        
        # Auto-detect format
        if format == 'auto':
            format = self._detect_format(model_path)
        
        # Load model
        model_file = next(model_path.glob(f"*.{format}"), None)
        if not model_file:
            raise FileNotFoundError(f"No .{format} file found in {model_path}")
        
        model = self.deserializers[format](model_file)
        
        # Load metadata
        metadata_file = model_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
        else:
            metadata = {}
        
        # Validate model
        self._validate_model(model, metadata)
        
        return model, metadata
    
    def _create_model_readme(self, model, metadata: dict, 
                            readme_file: Path) -> None:
        """Create comprehensive model documentation"""
        
        readme_content = f"""# Model: {metadata.get('model_name', 'Unknown')}
 ```       
## Model Details
- **Type**: {metadata.get('model_type', 'Unknown')}
- **Format**: {metadata.get('format', 'Unknown')}
- **Created**: {metadata.get('timestamp', 'Unknown')}
- **Python Version**: {metadata.get('python_version', 'Unknown')}

## Performance Metrics

```json
{json.dumps(metadata.get('performance_metrics', {}), indent=2)}
```

## Model Parameters
```json
{json.dumps(metadata.get('model_params', {}), indent=2)}
```

## Feature Importance
```
{self._format_feature_importance(metadata)}
```

## Usage Example
```python
import joblib
import pandas as pd

# Load model
model, metadata = joblib.load('model.joblib')

# Prepare data
X_new = pd.DataFrame(...)  # Same features as training

# Make predictions
predictions = model.predict(X_new)
```

## Dependencies
```
{json.dumps(metadata.get('library_versions', {}), indent=2)}
```

## Notes
- Model trained on {metadata.get('training_samples', 'Unknown')} samples
- Feature scaling: {metadata.get('feature_scaling', 'Unknown')}
- Cross-validation: {metadata.get('cv_folds', 'Unknown')} folds
"""
        
        with open(readme_file, 'w') as f:
            f.write(readme_content)


### **Model Management Examples**

```bash
# Save model with metadata
python src/data_vista.py \
  --model-save \
  --model-path models/churn_predictor \
  --format joblib \
  --metadata '{"version": "1.0", "author": "DataVista"}'

# Load and validate model
python src/data_vista.py \
  --model-load models/churn_predictor_20241022_190633 \
  --validate \
  --test-data data/test_set.csv

# Compare multiple model versions
python src/data_vista.py \
  --model-compare \
  --models models/*_2024* \
  --metric roc_auc \
  --output comparison_report.html

# Create model registry
python src/data_vista.py \
  --model-registry \
  --registry models/registry.db \
  --scan models/
```

## ðŸ”¬ Task 5: Clustering Analysis

### **Advanced Clustering Implementation**

```python
# clustering.py - Comprehensive Clustering System
class AdvancedClustering:
    """Multi-algorithm clustering with validation"""
    
    def __init__(self):
        from sklearn.cluster import (
            KMeans, DBSCAN, AgglomerativeClustering,
            SpectralClustering, OPTICS, Birch
        )
        from sklearn.mixture import GaussianMixture
        import hdbscan
        
        self.algorithms = {
            'kmeans': KMeans,
            'dbscan': DBSCAN,
            'hierarchical': AgglomerativeClustering,
            'gmm': GaussianMixture,
            'spectral': SpectralClustering,
            'optics': OPTICS,
            'birch': Birch,
            'hdbscan': hdbscan.HDBSCAN
        }
    
    def perform_clustering(self, X: pd.DataFrame, 
                          algorithm: str = 'auto',
                          n_clusters: int = None) -> dict:
        """Perform clustering with automatic algorithm selection"""
        
        # Preprocessing
        X_scaled = self._preprocess_data(X)
        
        # Algorithm selection
        if algorithm == 'auto':
            algorithm = self._select_best_algorithm(X_scaled)
        
        # Parameter tuning
        best_params = self._tune_clustering_params(X_scaled, algorithm)
        
        # Perform clustering
        clusters = self._cluster_data(X_scaled, algorithm, best_params)
        
        # Cluster validation
        validation_metrics = self._validate_clusters(X_scaled, clusters)
        
        # Cluster analysis
        cluster_analysis = self._analyze_clusters(X, clusters)
        
        # Visualization
        self._visualize_clusters(X_scaled, clusters, algorithm)
        
        return {
            'algorithm': algorithm,
            'parameters': best_params,
            'clusters': clusters,
            'validation_metrics': validation_metrics,
            'cluster_analysis': cluster_analysis,
            'n_clusters': len(np.unique(clusters)),
            'cluster_sizes': np.bincount(clusters[clusters >= 0])
        }
    
    def _tune_clustering_params(self, X: np.ndarray, 
                               algorithm: str) -> dict:
        """Automatic parameter tuning for clustering"""
        
        if algorithm == 'kmeans':
            # Elbow method for optimal k
            inertias = []
            k_range = range(2, min(20, len(X) // 10))
            
            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42)
                kmeans.fit(X)
                inertias.append(kmeans.inertia_)
            
            # Find elbow point
            optimal_k = self._find_elbow_point(k_range, inertias)
            return {'n_clusters': optimal_k}
        
        elif algorithm == 'dbscan':
            # Tune eps and min_samples
            from sklearn.neighbors import NearestNeighbors
            
            # Calculate k-distance graph
            neighbors = NearestNeighbors(n_neighbors=5)
            neighbors_fit = neighbors.fit(X)
            distances, indices = neighbors_fit.kneighbors(X)
            distances = np.sort(distances, axis=0)[:, 4]
            
            # Find knee point
            eps = self._find_knee_point(distances)
            min_samples = 5  # Default
            
            return {'eps': eps, 'min_samples': min_samples}
        
        elif algorithm == 'gmm':
            # Use BIC to select number of components
            n_components_range = range(1, 11)
            bics = []
            
            for n_components in n_components_range:
                gmm = GaussianMixture(n_components=n_components, 
                                     random_state=42)
                gmm.fit(X)
                bics.append(gmm.bic(X))
            
            optimal_n = n_components_range[np.argmin(bics)]
            return {'n_components': optimal_n}
```

### **Clustering Examples**

```bash
# Perform K-means clustering
python src/data_vista.py \
  --data data/customer_segments.csv \
  --cluster kmeans \
  --k auto \
  --output clusters/

# Compare multiple clustering algorithms
python src/data_vista.py \
  --data data/products.csv \
  --cluster compare \
  --algorithms kmeans dbscan hierarchical \
  --metrics silhouette davies_bouldin

# Hierarchical clustering with dendrogram
python src/data_vista.py \
  --data data/genetic_data.csv \
  --cluster hierarchical \
  --linkage ward \
  --plot dendrogram \
  --cut-height 0.5

# DBSCAN with automatic parameter tuning
python src/data_vista.py \
  --data data/anomaly_detection.csv \
  --cluster dbscan \
  --auto-tune \
  --min-samples auto
```

## ðŸ“ˆ Task 6: Time Series Forecasting

### **Complete Time Series Analysis**

```python
# time_series.py - Advanced Forecasting System
class TimeSeriesForecaster:
    """Comprehensive time series analysis and forecasting"""
    
    def __init__(self):
        import statsmodels.api as sm
        from statsmodels.tsa.arima.model import ARIMA
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        from prophet import Prophet
        
        self.models = {
            'arima': ARIMA,
            'sarima': SARIMAX,
            'exponential_smoothing': ExponentialSmoothing,
            'prophet': Prophet,
            'var': sm.tsa.VAR,
            'bvar': sm.tsa.VAR
        }
    
    def forecast(self, series: pd.Series, 
                model_type: str = 'auto',
                periods: int = 12,
                frequency: str = None) -> dict:
        """Perform time series forecasting"""
        
        # Data preparation
        series_clean = self._prepare_series(series, frequency)
        
        # Model selection
        if model_type == 'auto':
            model_type = self._select_best_model(series_clean)
        
        # Model training
        model = self._train_model(series_clean, model_type)
        
        # Forecasting
        forecast = self._generate_forecast(model, periods)
        
        # Model evaluation
        evaluation = self._evaluate_forecast(model, series_clean)
        
        # Decomposition
        decomposition = self._decompose_series(series_clean)
        
        # Anomaly detection
        anomalies = self._detect_anomalies(series_clean)
        
        return {
            'model': model,
            'model_type': model_type,
            'forecast': forecast,
            'evaluation': evaluation,
            'decomposition': decomposition,
            'anomalies': anomalies,
            'periods': periods,
            'frequency': frequency or series.index.inferred_freq
        }
    
    def _select_best_model(self, series: pd.Series) -> str:
        """Automatically select best forecasting model"""
        
        # Check stationarity
        stationarity_test = self._test_stationarity(series)
        
        # Check seasonality
        seasonality_test = self._test_seasonality(series)
        
        # Check trend
        trend_test = self._test_trend(series)
        
        # Select model based on characteristics
        if seasonality_test['has_seasonality']:
            if trend_test['has_trend']:
                return 'sarima'  # Seasonal ARIMA with trend
            else:
                return 'sarima'  # Seasonal ARIMA
        elif trend_test['has_trend']:
            if not stationarity_test['is_stationary']:
                return 'exponential_smoothing'  # Handle trend
            else:
                return 'arima'
        else:
            if len(series) > 1000:
                return 'prophet'  # Prophet for large datasets
            else:
                return 'arima'
    
    def _train_model(self, series: pd.Series, 
                    model_type: str):
        """Train selected forecasting model"""
        
        if model_type == 'arima':
            # Auto-ARIMA to find optimal parameters
            import pmdarima as pm
            
            model = pm.auto_arima(
                series,
                start_p=1, start_q=1,
                max_p=5, max_q=5,
                d=None,  # Let model determine
                seasonal=False,
                trace=True,
                error_action='ignore',
                suppress_warnings=True,
                stepwise=True
            )
            
        elif model_type == 'sarima':
            # Seasonal ARIMA
            import pmdarima as pm
            
            model = pm.auto_arima(
                series,
                start_p=1, start_q=1,
                max_p=5, max_q=5,
                m=12,  # Monthly seasonality
                d=None,
                seasonal=True,
                trace=True,
                error_action='ignore',
                suppress_warnings=True,
                stepwise=True
            )
            
        elif model_type == 'prophet':
            # Facebook Prophet
            from prophet import Prophet
            
            # Prepare data for Prophet
            df = pd.DataFrame({
                'ds': series.index,
                'y': series.values
            })
            
            model = Prophet(
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False,
                seasonality_mode='multiplicative',
                changepoint_prior_scale=0.05
            )
            
            model.fit(df)
        
        return model
```

### **Time Series Examples**

```bash
# ARIMA forecasting
python src/data_vista.py \
  --data data/sales_time_series.csv \
  --forecast arima \
  --periods 24 \
  --frequency M \
  --output forecast/

# Prophet with holidays
python src/data_vista.py \
  --data data/website_traffic.csv \
  --forecast prophet \
  --holidays data/holidays.csv \
  --seasonality yearly weekly \
  --output prophet_forecast/

# Multiple time series
python src/data_vista.py \
  --data data/multivariate_ts.csv \
  --forecast var \
  --variables sales traffic conversions \
  --lags 12 \
  --output multivariate/

# Anomaly detection
python src/data_vista.py \
  --data data/server_metrics.csv \
  --forecast detect-anomalies \
  --method isolation_forest \
  --contamination 0.01 \
  --output anomalies/
```

## ðŸ”¬ Task 7: Hypothesis Testing

### **Comprehensive Statistical Testing**

```python
# hypothesis_testing.py - Advanced Testing Framework
class HypothesisTestingSuite:
    """Comprehensive statistical testing capabilities"""
    
    def __init__(self):
        from scipy import stats
        import statsmodels.api as sm
        from statsmodels.stats import weightstats
        from statsmodels.stats.proportion import proportions_ztest
        
        self.tests = {
            'parametric': {
                't_test': {
                    'one_sample': stats.ttest_1samp,
                    'two_sample_independent': stats.ttest_ind,
                    'two_sample_paired': stats.ttest_rel
                },
                'anova': {
                    'one_way': stats.f_oneway,
                    'two_way': self._two_way_anova,
                    'repeated_measures': self._repeated_measures_anova
                },
                'correlation': {
                    'pearson': stats.pearsonr,
                    'partial': self._partial_correlation,
                    'multiple': self._multiple_correlation
                }
            },
            'non_parametric': {
                'mann_whitney': stats.mannwhitneyu,
                'wilcoxon': stats.wilcoxon,
                'kruskal_wallis': stats.kruskal,
                'friedman': stats.friedmanchisquare,
                'chi_square': stats.chisquare,
                'fisher_exact': stats.fisher_exact,
                'spearman': stats.spearmanr,
                'kendall': stats.kendalltau
            },
            'proportion': {
                'z_test': proportions_ztest,
                'chi_square_proportions': self._chi_square_proportions
            },
            'time_series': {
                'adf': sm.tsa.adfuller,
                'kpss': sm.tsa.kpss,
                'ljung_box': sm.stats.acorr_ljungbox,
                'granger': self._granger_causality
            }
        }
    
    def perform_test(self, data: pd.DataFrame, 
                    test_type: str, 
                    **kwargs) -> dict:
        """Perform comprehensive hypothesis testing"""
        
        # Check assumptions
        assumptions = self._check_assumptions(data, test_type, **kwargs)
        
        # Perform test
        if test_type in self.tests['parametric']:
            if not assumptions['parametric_assumptions_met']:
                warnings.warn("Parametric assumptions not met. Consider non-parametric alternative.")
        
        test_result = self._execute_test(data, test_type, **kwargs)
        
        # Calculate effect size
        effect_size = self._calculate_effect_size(data, test_type, test_result)
        
        # Calculate power
        power = self._calculate_power(data, test_type, test_result, 
                                     kwargs.get('alpha', 0.05))
        
        # Generate interpretation
        interpretation = self._interpret_results(test_result, effect_size, power)
        
        return {
            'test_type': test_type,
            'test_result': test_result,
            'assumptions': assumptions,
            'effect_size': effect_size,
            'statistical_power': power,
            'interpretation': interpretation,
            'recommendations': self._generate_recommendations(test_result, assumptions)
        }
    
    def _check_assumptions(self, data: pd.DataFrame, 
                          test_type: str, **kwargs) -> dict:
        """Check statistical test assumptions"""
        
        assumptions = {
            'normality': {},
            'homogeneity_of_variance': {},
            'independence': {},
            'linearity': {},
            'homoscedasticity': {},
            'multicollinearity': {},
            'stationarity': {}
        }
        
        # Check normality for parametric tests
        if test_type in ['t_test', 'anova', 'pearson']:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                stat, p = stats.shapiro(data[col].dropna())
                assumptions['normality'][col] = {
                    'statistic': stat,
                    'p_value': p,
                    'is_normal': p > 0.05
                }
        
        # Check homogeneity of variance for ANOVA
        if test_type == 'anova':
            group_col = kwargs.get('group_col')
            value_col = kwargs.get('value_col')
            
            if group_col and value_col:
                groups = [data[data[group_col] == g][value_col] 
                         for g in data[group_col].unique()]
                stat, p = stats.levene(*groups)
                assumptions['homogeneity_of_variance'] = {
                    'statistic': stat,
                    'p_value': p,
                    'equal_variance': p > 0.05
                }
        
        return assumptions
    
    def _calculate_effect_size(self, data: pd.DataFrame, 
                              test_type: str, 
                              test_result: dict) -> dict:
        """Calculate appropriate effect size measures"""
        
        effect_sizes = {}
        
        if test_type == 't_test':
            # Cohen's d
            group1 = kwargs.get('group1')
            group2 = kwargs.get('group2')
            
            if group1 is not None and group2 is not None:
                n1, n2 = len(group1), len(group2)
                pooled_std = np.sqrt(((n1-1)*np.var(group1) + (n2-1)*np.var(group2)) / (n1+n2-2))
                cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std
                
                effect_sizes['cohens_d'] = cohens_d
                effect_sizes['interpretation'] = self._interpret_cohens_d(cohens_d)
        
        elif test_type == 'anova':
            # Eta squared
            ss_between = test_result.get('ss_between', 0)
            ss_total = test_result.get('ss_total', 0)
            
            if ss_total > 0:
                eta_squared = ss_between / ss_total
                effect_sizes['eta_squared'] = eta_squared
                effect_sizes['interpretation'] = self._interpret_eta_squared(eta_squared)
        
        elif test_type == 'chi_square':
            # Cramer's V
            chi2 = test_result.get('statistic', 0)
            n = len(data)
            k = min(data.shape) - 1
            
            if n > 0 and k > 0:
                cramers_v = np.sqrt(chi2 / (n * k))
                effect_sizes['cramers_v'] = cramers_v
                effect_sizes['interpretation'] = self._interpret_cramers_v(cramers_v)
        
        return effect_sizes
```

### **Hypothesis Testing Examples**

```bash
# A/B testing
python src/data_vista.py \
  --data data/ab_test_results.csv \
  --test ttest \
  --group-a control \
  --group-b treatment \
  --metric conversion_rate \
  --power-analysis \
  --output ab_test_report/

# Chi-square test
python src/data_vista.py \
  --data data/survey_responses.csv \
  --test chi2 \
  --variable1 gender \
  --variable2 preference \
  --expected data/expected_frequencies.csv \
  --output chi2_results/

# ANOVA with post-hoc
python src/data_vista.py \
  --data data/experiment_groups.csv \
  --test anova \
  --group treatment_group \
  --metric test_score \
  --post-hoc tukey \
  --output anova_analysis/

# Correlation analysis
python src/data_vista.py \
  --data data/variable_pairs.csv \
  --test correlation \
  --variables price sales marketing_spend \
  --method spearman \
  --output correlation_matrix/
```

## ðŸš€ Task 8: Advanced Features

### **AutoML & Model Generation**

```python
# model_generater.py - Automated Machine Learning
class AutoMLGenerator:
    """Complete AutoML system with feature engineering"""
    
    def __init__(self):
        import autosklearn.classification
        import autosklearn.regression
        import tpot
        from h2o.automl import H2OAutoML
        
        self.automl_libraries = {
            'autosklearn': {
                'classification': autosklearn.classification.AutoSklearnClassifier,
                'regression': autosklearn.regression.AutoSklearnRegressor
            },
            'tpot': {
                'classification': tpot.TPOTClassifier,
                'regression': tpot.TPOTRegressor
            },
            'h2o': H2OAutoML
        }
    
    def generate_model(self, X: pd.DataFrame, y: pd.Series,
                      task_type: str = 'auto',
                      time_limit: int = 3600,
                      ensemble_size: int = 50) -> dict:
        """Generate optimal model using AutoML"""
        
        # Auto-detect task type
        if task_type == 'auto':
            task_type = self._detect_task_type(y)
        
        # Feature engineering
        X_engineered = self._automated_feature_engineering(X, y, task_type)
        
        # Feature selection
        X_selected = self._automated_feature_selection(X_engineered, y, task_type)
        
        # Run AutoML
        automl_results = self._run_automl(X_selected, y, task_type, 
                                         time_limit, ensemble_size)
        
        # Ensemble creation
        ensemble = self._create_ensemble(automl_results['models'])
        
        # Model interpretation
        interpretation = self._interpret_automl_results(automl_results)
        
        # Deployment artifacts
        artifacts = self._create_deployment_artifacts(ensemble, X_selected)
        
        return {
            'best_model': automl_results['best_model'],
            'ensemble': ensemble,
            'leaderboard': automl_results['leaderboard'],
            'interpretation': interpretation,
            'artifacts': artifacts,
            'task_type': task_type,
            'feature_importance': automl_results['feature_importance'],
            'performance_metrics': automl_results['performance_metrics']
        }
    
    def _automated_feature_engineering(self, X: pd.DataFrame, 
                                      y: pd.Series, 
                                      task_type: str) -> pd.DataFrame:
        """Automated feature engineering pipeline"""
        
        import featuretools as ft
        
        # Create entity set
        es = ft.EntitySet(id='data')
        es = es.add_dataframe(
            dataframe_name='data',
            dataframe=X,
            index='index',
            make_index=True
        )
        
        # Automated feature engineering
        feature_matrix, feature_defs = ft.dfs(
            entityset=es,
            target_dataframe_name='data',
            max_depth=2,
            verbose=True,
            n_jobs=-1,
            features_only=False
        )
        
        # Add statistical features
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            # Statistical transformations
            X[f'{col}_log'] = np.log1p(X[col])
            X[f'{col}_sqrt'] = np.sqrt(X[col])
            X[f'{col}_square'] = X[col] ** 2
            X[f'{col}_cube'] = X[col] ** 3
            
            # Rolling statistics
            if len(X) > 10:
                X[f'{col}_rolling_mean_5'] = X[col].rolling(5).mean()
                X[f'{col}_rolling_std_5'] = X[col].rolling(5).std()
        
        # Interaction features
        if len(numeric_cols) > 1:
            for i in range(len(numeric_cols)):
                for j in range(i+1, len(numeric_cols)):
                    col1, col2 = numeric_cols[i], numeric_cols[j]
                    X[f'{col1}_times_{col2}'] = X[col1] * X[col2]
                    X[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-10)
        
        return X
```

## ðŸ”— Task 9: Integration & Automation

### **Complete Pipeline Orchestration**

```python
# pipeline.py - End-to-End Data Pipeline
class DataVistaPipeline:
    """Complete data science pipeline orchestration"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.modules = {
            'loader': DataLoader(),
            'cleaner': DataCleaner(),
            'preprocessor': DataPreprocessor(),
            'analyzer': StatisticalAnalysis(),
            'tester': HypothesisTesting(),
            'ml': MachineLearning(),
            'visualizer': Visualization(),
            'forecaster': TimeSeriesForecaster(),
            'clusterer': AdvancedClustering(),
            'automl': AutoMLGenerator()
        }
    
    def run_complete_analysis(self, data_path: str, 
                             output_dir: str = None) -> dict:
        """Run complete end-to-end analysis pipeline"""
        
        import time
        start_time = time.time()
        
        # Create output directory
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
        
        # 1. Data Loading
        print("ðŸ“¥ Step 1: Loading data...")
        data = self.modules['loader'].load(data_path)
        
        # 2. Data Cleaning
        print("ðŸ§¹ Step 2: Cleaning data...")
        clean_data = self.modules['cleaner'].clean(data)
        
        # 3. Statistical Analysis
        print("ðŸ“Š Step 3: Statistical analysis...")
        stats_results = self.modules['analyzer'].analyze(clean_data)
        
        # 4. Hypothesis Testing
        print("ðŸ”¬ Step 4: Hypothesis testing...")
        test_results = self.modules['tester'].perform_comprehensive_tests(clean_data)
        
        # 5. Machine Learning
        print("ðŸ¤– Step 5: Machine learning...")
        ml_results = self.modules['ml'].run_complete_ml_pipeline(clean_data)
        
        # 6. Visualization
        print("ðŸ“ˆ Step 6: Creating visualizations...")
        viz_results = self.modules['visualizer'].create_comprehensive_dashboard(
            clean_data, {**stats_results, **ml_results}
        )
        
        # 7. Generate Report
        print("ðŸ“‹ Step 7: Generating report...")
        report = self._generate_comprehensive_report(
            data_path, clean_data, stats_results, 
            test_results, ml_results, viz_results
        )
        
        # Calculate runtime
        runtime = time.time() - start_time
        
        # Save results
        if output_dir:
            self._save_results(output_path, {
                'data': clean_data,
                'statistics': stats_results,
                'tests': test_results,
                'ml': ml_results,
                'visualizations': viz_results,
                'report': report,
                'runtime': runtime
            })
        
        return {
            'status': 'success',
            'runtime': runtime,
            'data_summary': {
                'original_rows': len(data),
                'cleaned_rows': len(clean_data),
                'features': list(clean_data.columns),
                'data_types': clean_data.dtypes.to_dict()
            },
            'insights': self._extract_key_insights(
                stats_results, test_results, ml_results
            ),
            'recommendations': self._generate_recommendations(
                stats_results, test_results, ml_results
            ),
            'output_files': self._list_output_files(output_path) if output_dir else []
        }
```

## ðŸ“š Conclusion

DataVista provides a **comprehensive, production-ready data science platform** that empowers users at all skill levels. From data loading and cleaning to advanced machine learning and visualization, every aspect of the data science workflow is covered with professional-grade implementations.

### **Key Strengths:**

1. **Modular Architecture**: Each component is independent yet seamlessly integrated
2. **Professional Implementation**: Production-quality code with comprehensive error handling
3. **Extensive Documentation**: Complete walkthroughs, examples, and best practices
4. **Scalable Performance**: Optimized for both small datasets and big data scenarios
5. **Enterprise-Ready**: Includes logging, monitoring, and deployment capabilities

### **Getting Help:**

```bash
# View help
python src/data_vista.py --help

# View module-specific help
python src/data_vista.py --module machine_learning --help

# View examples
python src/data_vista.py --examples

# Get version info
python src/data_vista.py --version
```

### **Community & Support:**

- **GitHub Issues**: [Report bugs or request features](https://github.com/Willie-Conway/DataVista-Command-Line-Application/issues)
- **Documentation**: [Complete user guide](https://github.com/Willie-Conway/DataVista-Command-Line-Application/tree/main/docs)
- **Examples**: [Sample workflows and use cases](https://github.com/Willie-Conway/DataVista-Command-Line-Application/tree/main/examples)

---

**DataVista** - Your complete data science companion! Transform raw data into actionable insights with confidence and precision. ðŸš€

*Last Updated: January 28, 2025*  
*Version: 2.0.0*  
*Author: Willie Conway*  
*License: MIT*
